{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MPGNN: Molecule Prediction with Graph Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Torch version: 2.0.1\n",
      "Cuda available: False\n",
      "Torch geometric version: 2.3.1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from rdkit import Chem\n",
    "import torch\n",
    "import torch_geometric\n",
    "from torch_geometric.data import Dataset, Data\n",
    "import numpy as np \n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(f\"Torch version: {torch.__version__}\")\n",
    "print(f\"Cuda available: {torch.cuda.is_available()}\")\n",
    "print(f\"Torch geometric version: {torch_geometric.__version__}\")\n",
    "\n",
    "\"\"\"\n",
    "!!!\n",
    "NOTE: This file was replaced by dataset_featurizer.py\n",
    "but is kept to illustrate how to build a custom dataset in PyG.\n",
    "!!!\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class MoleculeDataset(Dataset):\n",
    "    def __init__(self, root, filename, test=False, transform=None, pre_transform=None):\n",
    "        \"\"\"\n",
    "        root = Where the dataset should be stored. This folder is split\n",
    "        into raw_dir (downloaded dataset) and processed_dir (processed data). \n",
    "        \"\"\"\n",
    "        self.test = test\n",
    "        self.filename = filename\n",
    "        super(MoleculeDataset, self).__init__(root, transform, pre_transform)\n",
    "        \n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        \"\"\" If this file exists in raw_dir, the download is not triggered.\n",
    "            (The download func. is not implemented here)  \n",
    "        \"\"\"\n",
    "        return self.filename\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        \"\"\" If these files are found in raw_dir, processing is skipped\"\"\"\n",
    "        self.data = pd.read_csv(self.raw_paths[0]).reset_index()\n",
    "\n",
    "        if self.test:\n",
    "            return [f'data_test_{i}.pt' for i in list(self.data.index)]\n",
    "        else:\n",
    "            return [f'data_{i}.pt' for i in list(self.data.index)]\n",
    "\n",
    "    def download(self):\n",
    "        pass\n",
    "\n",
    "    def process(self):\n",
    "        self.data = pd.read_csv(self.raw_paths[0])\n",
    "        for index, mol in tqdm(self.data.iterrows(), total=self.data.shape[0]):\n",
    "            mol_obj = Chem.MolFromSmiles(mol[\"smiles\"])\n",
    "            # Get node features\n",
    "            node_feats = self._get_node_features(mol_obj)\n",
    "            # Get edge features\n",
    "            edge_feats = self._get_edge_features(mol_obj)\n",
    "            # Get adjacency info\n",
    "            edge_index = self._get_adjacency_info(mol_obj)\n",
    "            # Get labels info\n",
    "            label = self._get_labels(mol[\"HIV_active\"])\n",
    "\n",
    "            # Create data object\n",
    "            data = Data(x=node_feats, \n",
    "                        edge_index=edge_index,\n",
    "                        edge_attr=edge_feats,\n",
    "                        y=label,\n",
    "                        smiles=mol[\"smiles\"]\n",
    "                        ) \n",
    "            if self.test:\n",
    "                torch.save(data, \n",
    "                    os.path.join(self.processed_dir, \n",
    "                                 f'data_test_{index}.pt'))\n",
    "            else:\n",
    "                torch.save(data, \n",
    "                    os.path.join(self.processed_dir, \n",
    "                                 f'data_{index}.pt'))\n",
    "\n",
    "    def _get_node_features(self, mol):\n",
    "        \"\"\" \n",
    "        This will return a matrix / 2d array of the shape\n",
    "        [Number of Nodes, Node Feature size]\n",
    "        \"\"\"\n",
    "        all_node_feats = []\n",
    "\n",
    "        for atom in mol.GetAtoms():\n",
    "            node_feats = []\n",
    "            # Feature 1: Atomic number        \n",
    "            node_feats.append(atom.GetAtomicNum())\n",
    "            # Feature 2: Atom degree\n",
    "            node_feats.append(atom.GetDegree())\n",
    "            # Feature 3: Formal charge\n",
    "            node_feats.append(atom.GetFormalCharge())\n",
    "            # Feature 4: Hybridization\n",
    "            node_feats.append(atom.GetHybridization())\n",
    "            # Feature 5: Aromaticity\n",
    "            node_feats.append(atom.GetIsAromatic())\n",
    "            # Feature 6: Total Num Hs\n",
    "            node_feats.append(atom.GetTotalNumHs())\n",
    "            # Feature 7: Radical Electrons\n",
    "            node_feats.append(atom.GetNumRadicalElectrons())\n",
    "            # Feature 8: In Ring\n",
    "            node_feats.append(atom.IsInRing())\n",
    "            # Feature 9: Chirality\n",
    "            node_feats.append(atom.GetChiralTag())\n",
    "\n",
    "            # Append node features to matrix\n",
    "            all_node_feats.append(node_feats)\n",
    "\n",
    "        all_node_feats = np.asarray(all_node_feats)\n",
    "        return torch.tensor(all_node_feats, dtype=torch.float)\n",
    "\n",
    "    def _get_edge_features(self, mol):\n",
    "        \"\"\" \n",
    "        This will return a matrix / 2d array of the shape\n",
    "        [Number of edges, Edge Feature size]\n",
    "        \"\"\"\n",
    "        all_edge_feats = []\n",
    "\n",
    "        for bond in mol.GetBonds():\n",
    "            edge_feats = []\n",
    "            # Feature 1: Bond type (as double)\n",
    "            edge_feats.append(bond.GetBondTypeAsDouble())\n",
    "            # Feature 2: Rings\n",
    "            edge_feats.append(bond.IsInRing())\n",
    "            # Append node features to matrix (twice, per direction)\n",
    "            all_edge_feats += [edge_feats, edge_feats]\n",
    "\n",
    "        all_edge_feats = np.asarray(all_edge_feats)\n",
    "        return torch.tensor(all_edge_feats, dtype=torch.float)\n",
    "\n",
    "    def _get_adjacency_info(self, mol):\n",
    "        \"\"\"\n",
    "        We could also use rdmolops.GetAdjacencyMatrix(mol)\n",
    "        but we want to be sure that the order of the indices\n",
    "        matches the order of the edge features\n",
    "        \"\"\"\n",
    "        edge_indices = []\n",
    "        for bond in mol.GetBonds():\n",
    "            i = bond.GetBeginAtomIdx()\n",
    "            j = bond.GetEndAtomIdx()\n",
    "            edge_indices += [[i, j], [j, i]]\n",
    "\n",
    "        edge_indices = torch.tensor(edge_indices)\n",
    "        edge_indices = edge_indices.t().to(torch.long).view(2, -1)\n",
    "        return edge_indices\n",
    "\n",
    "    def _get_labels(self, label):\n",
    "        label = np.asarray([label])\n",
    "        return torch.tensor(label, dtype=torch.int64)\n",
    "\n",
    "    def len(self):\n",
    "        return self.data.shape[0]\n",
    "\n",
    "    def get(self, idx):\n",
    "        \"\"\" - Equivalent to __getitem__ in pytorch\n",
    "            - Is not needed for PyG's InMemoryDataset\n",
    "        \"\"\"\n",
    "        if self.test:\n",
    "            data = torch.load(os.path.join(self.processed_dir, \n",
    "                                 f'data_test_{idx}.pt'))\n",
    "        else:\n",
    "            data = torch.load(os.path.join(self.processed_dir, \n",
    "                                 f'data_{idx}.pt'))   \n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n",
      " 27%|██▋       | 19569/71634 [01:43<04:39, 186.58it/s][16:09:35] WARNING: not removing hydrogen atom without neighbors\n",
      "[16:09:35] WARNING: not removing hydrogen atom without neighbors\n",
      "100%|██████████| 71634/71634 [06:24<00:00, 186.13it/s]\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "dataset = MoleculeDataset(root=\"./data/HIV/\", filename=\"HIV_train_oversampled.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1],\n",
       "        [ 1,  0],\n",
       "        [ 1,  2],\n",
       "        [ 2,  1],\n",
       "        [ 2,  3],\n",
       "        [ 3,  2],\n",
       "        [ 3,  4],\n",
       "        [ 4,  3],\n",
       "        [ 4,  5],\n",
       "        [ 5,  4],\n",
       "        [ 5,  6],\n",
       "        [ 6,  5],\n",
       "        [ 6,  7],\n",
       "        [ 7,  6],\n",
       "        [ 7,  8],\n",
       "        [ 8,  7],\n",
       "        [ 8,  9],\n",
       "        [ 9,  8],\n",
       "        [ 9, 10],\n",
       "        [10,  9],\n",
       "        [10, 11],\n",
       "        [11, 10],\n",
       "        [11, 12],\n",
       "        [12, 11],\n",
       "        [12, 13],\n",
       "        [13, 12],\n",
       "        [13, 14],\n",
       "        [14, 13],\n",
       "        [14, 15],\n",
       "        [15, 14],\n",
       "        [14, 16],\n",
       "        [16, 14],\n",
       "        [16, 17],\n",
       "        [17, 16],\n",
       "        [17, 18],\n",
       "        [18, 17],\n",
       "        [18, 19],\n",
       "        [19, 18],\n",
       "        [19, 20],\n",
       "        [20, 19],\n",
       "        [20, 21],\n",
       "        [21, 20],\n",
       "        [21, 22],\n",
       "        [22, 21],\n",
       "        [22, 23],\n",
       "        [23, 22],\n",
       "        [23, 24],\n",
       "        [24, 23],\n",
       "        [24, 25],\n",
       "        [25, 24],\n",
       "        [25, 26],\n",
       "        [26, 25],\n",
       "        [25, 27],\n",
       "        [27, 25],\n",
       "        [25, 28],\n",
       "        [28, 25],\n",
       "        [24, 29],\n",
       "        [29, 24],\n",
       "        [29, 30],\n",
       "        [30, 29],\n",
       "        [30, 31],\n",
       "        [31, 30],\n",
       "        [31, 32],\n",
       "        [32, 31],\n",
       "        [32, 33],\n",
       "        [33, 32],\n",
       "        [33, 34],\n",
       "        [34, 33],\n",
       "        [34, 35],\n",
       "        [35, 34],\n",
       "        [35, 36],\n",
       "        [36, 35],\n",
       "        [19, 37],\n",
       "        [37, 19],\n",
       "        [37, 38],\n",
       "        [38, 37],\n",
       "        [12, 39],\n",
       "        [39, 12],\n",
       "        [39, 40],\n",
       "        [40, 39],\n",
       "        [40, 41],\n",
       "        [41, 40],\n",
       "        [41, 42],\n",
       "        [42, 41],\n",
       "        [41, 43],\n",
       "        [43, 41],\n",
       "        [41, 44],\n",
       "        [44, 41],\n",
       "        [ 6, 45],\n",
       "        [45,  6],\n",
       "        [45, 46],\n",
       "        [46, 45],\n",
       "        [46, 47],\n",
       "        [47, 46],\n",
       "        [46, 48],\n",
       "        [48, 46],\n",
       "        [46, 49],\n",
       "        [49, 46],\n",
       "        [45, 50],\n",
       "        [50, 45],\n",
       "        [ 1, 51],\n",
       "        [51,  1],\n",
       "        [51, 52],\n",
       "        [52, 51],\n",
       "        [52, 53],\n",
       "        [53, 52],\n",
       "        [53, 54],\n",
       "        [54, 53],\n",
       "        [54, 55],\n",
       "        [55, 54],\n",
       "        [55, 56],\n",
       "        [56, 55],\n",
       "        [56, 57],\n",
       "        [57, 56],\n",
       "        [57, 58],\n",
       "        [58, 57],\n",
       "        [58, 59],\n",
       "        [59, 58],\n",
       "        [59, 60],\n",
       "        [60, 59],\n",
       "        [60, 61],\n",
       "        [61, 60],\n",
       "        [60, 62],\n",
       "        [62, 60],\n",
       "        [60, 63],\n",
       "        [63, 60],\n",
       "        [59, 64],\n",
       "        [64, 59],\n",
       "        [64, 65],\n",
       "        [65, 64],\n",
       "        [65, 66],\n",
       "        [66, 65],\n",
       "        [66, 67],\n",
       "        [67, 66],\n",
       "        [67, 68],\n",
       "        [68, 67],\n",
       "        [68, 69],\n",
       "        [69, 68],\n",
       "        [69, 70],\n",
       "        [70, 69],\n",
       "        [70, 71],\n",
       "        [71, 70],\n",
       "        [54, 72],\n",
       "        [72, 54],\n",
       "        [72, 73],\n",
       "        [73, 72],\n",
       "        [50,  3],\n",
       "        [ 3, 50],\n",
       "        [73, 51],\n",
       "        [51, 73],\n",
       "        [40,  9],\n",
       "        [ 9, 40],\n",
       "        [70, 57],\n",
       "        [57, 70],\n",
       "        [38, 16],\n",
       "        [16, 38],\n",
       "        [69, 64],\n",
       "        [64, 69],\n",
       "        [35, 22],\n",
       "        [22, 35],\n",
       "        [34, 29],\n",
       "        [29, 34]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0].edge_index.t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 8.,  1.,  0.,  3.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 6.,  3.,  0.,  3.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 7.,  2.,  0.,  3.,  0.,  1.,  0.,  0.,  0.],\n",
       "        [ 6.,  3.,  0.,  3.,  1.,  0.,  0.,  1.,  0.],\n",
       "        [ 6.,  2.,  0.,  3.,  1.,  1.,  0.,  1.,  0.],\n",
       "        [ 6.,  2.,  0.,  3.,  1.,  1.,  0.,  1.,  0.],\n",
       "        [ 6.,  3.,  0.,  3.,  1.,  0.,  0.,  1.,  0.],\n",
       "        [ 6.,  2.,  0.,  3.,  0.,  1.,  0.,  0.,  0.],\n",
       "        [ 6.,  2.,  0.,  3.,  0.,  1.,  0.,  0.,  0.],\n",
       "        [ 6.,  3.,  0.,  3.,  1.,  0.,  0.,  1.,  0.],\n",
       "        [ 6.,  2.,  0.,  3.,  1.,  1.,  0.,  1.,  0.],\n",
       "        [ 6.,  2.,  0.,  3.,  1.,  1.,  0.,  1.,  0.],\n",
       "        [ 6.,  3.,  0.,  3.,  1.,  0.,  0.,  1.,  0.],\n",
       "        [ 7.,  2.,  0.,  3.,  0.,  1.,  0.,  0.,  0.],\n",
       "        [ 6.,  3.,  0.,  3.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 8.,  1.,  0.,  3.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 6.,  3.,  0.,  3.,  1.,  0.,  0.,  1.,  0.],\n",
       "        [ 6.,  2.,  0.,  3.,  1.,  1.,  0.,  1.,  0.],\n",
       "        [ 6.,  2.,  0.,  3.,  1.,  1.,  0.,  1.,  0.],\n",
       "        [ 6.,  3.,  0.,  3.,  1.,  0.,  0.,  1.,  0.],\n",
       "        [ 7.,  2.,  0.,  3.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 7.,  2.,  0.,  3.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 6.,  3.,  0.,  3.,  1.,  0.,  0.,  1.,  0.],\n",
       "        [ 6.,  2.,  0.,  3.,  1.,  1.,  0.,  1.,  0.],\n",
       "        [ 6.,  3.,  0.,  3.,  1.,  0.,  0.,  1.,  0.],\n",
       "        [16.,  4.,  0.,  4.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 8.,  1.,  0.,  3.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 8.,  1.,  0.,  3.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 8.,  1.,  0.,  4.,  0.,  1.,  0.,  0.,  0.],\n",
       "        [ 6.,  3.,  0.,  3.,  1.,  0.,  0.,  1.,  0.],\n",
       "        [ 6.,  2.,  0.,  3.,  1.,  1.,  0.,  1.,  0.],\n",
       "        [ 6.,  2.,  0.,  3.,  1.,  1.,  0.,  1.,  0.],\n",
       "        [ 6.,  2.,  0.,  3.,  1.,  1.,  0.,  1.,  0.],\n",
       "        [ 7.,  2.,  0.,  3.,  1.,  0.,  0.,  1.,  0.],\n",
       "        [ 6.,  3.,  0.,  3.,  1.,  0.,  0.,  1.,  0.],\n",
       "        [ 6.,  3.,  0.,  3.,  1.,  0.,  0.,  1.,  0.],\n",
       "        [ 8.,  1.,  0.,  3.,  0.,  1.,  0.,  0.,  0.],\n",
       "        [ 6.,  2.,  0.,  3.,  1.,  1.,  0.,  1.,  0.],\n",
       "        [ 6.,  2.,  0.,  3.,  1.,  1.,  0.,  1.,  0.],\n",
       "        [ 6.,  2.,  0.,  3.,  1.,  1.,  0.,  1.,  0.],\n",
       "        [ 6.,  3.,  0.,  3.,  1.,  0.,  0.,  1.,  0.],\n",
       "        [16.,  4.,  0.,  4.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 8.,  1.,  0.,  3.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 8.,  1.,  0.,  3.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 8.,  1.,  0.,  4.,  0.,  1.,  0.,  0.,  0.],\n",
       "        [ 6.,  3.,  0.,  3.,  1.,  0.,  0.,  1.,  0.],\n",
       "        [16.,  4.,  0.,  4.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 8.,  1.,  0.,  3.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 8.,  1.,  0.,  3.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 8.,  1.,  0.,  4.,  0.,  1.,  0.,  0.,  0.],\n",
       "        [ 6.,  2.,  0.,  3.,  1.,  1.,  0.,  1.,  0.],\n",
       "        [ 6.,  3.,  0.,  3.,  1.,  0.,  0.,  1.,  0.],\n",
       "        [ 6.,  2.,  0.,  3.,  1.,  1.,  0.,  1.,  0.],\n",
       "        [ 6.,  2.,  0.,  3.,  1.,  1.,  0.,  1.,  0.],\n",
       "        [ 6.,  3.,  0.,  3.,  1.,  0.,  0.,  1.,  0.],\n",
       "        [ 7.,  2.,  0.,  3.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 7.,  2.,  0.,  3.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 6.,  3.,  0.,  3.,  1.,  0.,  0.,  1.,  0.],\n",
       "        [ 6.,  2.,  0.,  3.,  1.,  1.,  0.,  1.,  0.],\n",
       "        [ 6.,  3.,  0.,  3.,  1.,  0.,  0.,  1.,  0.],\n",
       "        [16.,  4.,  0.,  4.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 8.,  1.,  0.,  3.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 8.,  1.,  0.,  3.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 8.,  1.,  0.,  4.,  0.,  1.,  0.,  0.,  0.],\n",
       "        [ 6.,  3.,  0.,  3.,  1.,  0.,  0.,  1.,  0.],\n",
       "        [ 6.,  2.,  0.,  3.,  1.,  1.,  0.,  1.,  0.],\n",
       "        [ 6.,  2.,  0.,  3.,  1.,  1.,  0.,  1.,  0.],\n",
       "        [ 6.,  2.,  0.,  3.,  1.,  1.,  0.,  1.,  0.],\n",
       "        [ 7.,  2.,  0.,  3.,  1.,  0.,  0.,  1.,  0.],\n",
       "        [ 6.,  3.,  0.,  3.,  1.,  0.,  0.,  1.,  0.],\n",
       "        [ 6.,  3.,  0.,  3.,  1.,  0.,  0.,  1.,  0.],\n",
       "        [ 8.,  1.,  0.,  3.,  0.,  1.,  0.,  0.,  0.],\n",
       "        [ 6.,  2.,  0.,  3.,  1.,  1.,  0.,  1.,  0.],\n",
       "        [ 6.,  2.,  0.,  3.,  1.,  1.,  0.,  1.,  0.],\n",
       "        [11.,  0.,  0.,  1.,  0.,  1.,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0].x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.0000, 0.0000],\n",
       "        [2.0000, 0.0000],\n",
       "        [1.0000, 0.0000],\n",
       "        [1.0000, 0.0000],\n",
       "        [1.0000, 0.0000],\n",
       "        [1.0000, 0.0000],\n",
       "        [1.5000, 1.0000],\n",
       "        [1.5000, 1.0000],\n",
       "        [1.5000, 1.0000],\n",
       "        [1.5000, 1.0000],\n",
       "        [1.5000, 1.0000],\n",
       "        [1.5000, 1.0000],\n",
       "        [1.0000, 0.0000],\n",
       "        [1.0000, 0.0000],\n",
       "        [2.0000, 0.0000],\n",
       "        [2.0000, 0.0000],\n",
       "        [1.0000, 0.0000],\n",
       "        [1.0000, 0.0000],\n",
       "        [1.5000, 1.0000],\n",
       "        [1.5000, 1.0000],\n",
       "        [1.5000, 1.0000],\n",
       "        [1.5000, 1.0000],\n",
       "        [1.5000, 1.0000],\n",
       "        [1.5000, 1.0000],\n",
       "        [1.0000, 0.0000],\n",
       "        [1.0000, 0.0000],\n",
       "        [1.0000, 0.0000],\n",
       "        [1.0000, 0.0000],\n",
       "        [2.0000, 0.0000],\n",
       "        [2.0000, 0.0000],\n",
       "        [1.0000, 0.0000],\n",
       "        [1.0000, 0.0000],\n",
       "        [1.5000, 1.0000],\n",
       "        [1.5000, 1.0000],\n",
       "        [1.5000, 1.0000],\n",
       "        [1.5000, 1.0000],\n",
       "        [1.5000, 1.0000],\n",
       "        [1.5000, 1.0000],\n",
       "        [1.0000, 0.0000],\n",
       "        [1.0000, 0.0000],\n",
       "        [2.0000, 0.0000],\n",
       "        [2.0000, 0.0000],\n",
       "        [1.0000, 0.0000],\n",
       "        [1.0000, 0.0000],\n",
       "        [1.5000, 1.0000],\n",
       "        [1.5000, 1.0000],\n",
       "        [1.5000, 1.0000],\n",
       "        [1.5000, 1.0000],\n",
       "        [1.0000, 0.0000],\n",
       "        [1.0000, 0.0000],\n",
       "        [2.0000, 0.0000],\n",
       "        [2.0000, 0.0000],\n",
       "        [2.0000, 0.0000],\n",
       "        [2.0000, 0.0000],\n",
       "        [1.0000, 0.0000],\n",
       "        [1.0000, 0.0000],\n",
       "        [1.5000, 1.0000],\n",
       "        [1.5000, 1.0000],\n",
       "        [1.5000, 1.0000],\n",
       "        [1.5000, 1.0000],\n",
       "        [1.5000, 1.0000],\n",
       "        [1.5000, 1.0000],\n",
       "        [1.5000, 1.0000],\n",
       "        [1.5000, 1.0000],\n",
       "        [1.5000, 1.0000],\n",
       "        [1.5000, 1.0000],\n",
       "        [1.5000, 1.0000],\n",
       "        [1.5000, 1.0000],\n",
       "        [1.5000, 1.0000],\n",
       "        [1.5000, 1.0000],\n",
       "        [1.0000, 0.0000],\n",
       "        [1.0000, 0.0000],\n",
       "        [1.5000, 1.0000],\n",
       "        [1.5000, 1.0000],\n",
       "        [1.5000, 1.0000],\n",
       "        [1.5000, 1.0000],\n",
       "        [1.5000, 1.0000],\n",
       "        [1.5000, 1.0000],\n",
       "        [1.5000, 1.0000],\n",
       "        [1.5000, 1.0000],\n",
       "        [1.0000, 0.0000],\n",
       "        [1.0000, 0.0000],\n",
       "        [2.0000, 0.0000],\n",
       "        [2.0000, 0.0000],\n",
       "        [2.0000, 0.0000],\n",
       "        [2.0000, 0.0000],\n",
       "        [1.0000, 0.0000],\n",
       "        [1.0000, 0.0000],\n",
       "        [1.5000, 1.0000],\n",
       "        [1.5000, 1.0000],\n",
       "        [1.0000, 0.0000],\n",
       "        [1.0000, 0.0000],\n",
       "        [2.0000, 0.0000],\n",
       "        [2.0000, 0.0000],\n",
       "        [2.0000, 0.0000],\n",
       "        [2.0000, 0.0000],\n",
       "        [1.0000, 0.0000],\n",
       "        [1.0000, 0.0000],\n",
       "        [1.5000, 1.0000],\n",
       "        [1.5000, 1.0000],\n",
       "        [1.0000, 0.0000],\n",
       "        [1.0000, 0.0000],\n",
       "        [1.5000, 1.0000],\n",
       "        [1.5000, 1.0000],\n",
       "        [1.5000, 1.0000],\n",
       "        [1.5000, 1.0000],\n",
       "        [1.5000, 1.0000],\n",
       "        [1.5000, 1.0000],\n",
       "        [1.0000, 0.0000],\n",
       "        [1.0000, 0.0000],\n",
       "        [2.0000, 0.0000],\n",
       "        [2.0000, 0.0000],\n",
       "        [1.0000, 0.0000],\n",
       "        [1.0000, 0.0000],\n",
       "        [1.5000, 1.0000],\n",
       "        [1.5000, 1.0000],\n",
       "        [1.5000, 1.0000],\n",
       "        [1.5000, 1.0000],\n",
       "        [1.0000, 0.0000],\n",
       "        [1.0000, 0.0000],\n",
       "        [2.0000, 0.0000],\n",
       "        [2.0000, 0.0000],\n",
       "        [2.0000, 0.0000],\n",
       "        [2.0000, 0.0000],\n",
       "        [1.0000, 0.0000],\n",
       "        [1.0000, 0.0000],\n",
       "        [1.5000, 1.0000],\n",
       "        [1.5000, 1.0000],\n",
       "        [1.5000, 1.0000],\n",
       "        [1.5000, 1.0000],\n",
       "        [1.5000, 1.0000],\n",
       "        [1.5000, 1.0000],\n",
       "        [1.5000, 1.0000],\n",
       "        [1.5000, 1.0000],\n",
       "        [1.5000, 1.0000],\n",
       "        [1.5000, 1.0000],\n",
       "        [1.5000, 1.0000],\n",
       "        [1.5000, 1.0000],\n",
       "        [1.5000, 1.0000],\n",
       "        [1.5000, 1.0000],\n",
       "        [1.0000, 0.0000],\n",
       "        [1.0000, 0.0000],\n",
       "        [1.5000, 1.0000],\n",
       "        [1.5000, 1.0000],\n",
       "        [1.5000, 1.0000],\n",
       "        [1.5000, 1.0000],\n",
       "        [1.5000, 1.0000],\n",
       "        [1.5000, 1.0000],\n",
       "        [1.5000, 1.0000],\n",
       "        [1.5000, 1.0000],\n",
       "        [1.5000, 1.0000],\n",
       "        [1.5000, 1.0000],\n",
       "        [1.5000, 1.0000],\n",
       "        [1.5000, 1.0000],\n",
       "        [1.5000, 1.0000],\n",
       "        [1.5000, 1.0000],\n",
       "        [1.5000, 1.0000],\n",
       "        [1.5000, 1.0000],\n",
       "        [1.5000, 1.0000],\n",
       "        [1.5000, 1.0000],\n",
       "        [1.5000, 1.0000],\n",
       "        [1.5000, 1.0000]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0].edge_attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0].y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F \n",
    "from torch.nn import Linear, BatchNorm1d, ModuleList\n",
    "from torch_geometric.nn import TransformerConv, TopKPooling \n",
    "from torch_geometric.nn import global_mean_pool as gap, global_max_pool as gmp\n",
    "torch.manual_seed(42)\n",
    "\n",
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, feature_size, model_params):\n",
    "        super(GNN, self).__init__()\n",
    "        embedding_size = model_params[\"model_embedding_size\"]\n",
    "        n_heads = model_params[\"model_attention_heads\"]\n",
    "        self.n_layers = model_params[\"model_layers\"]\n",
    "        dropout_rate = model_params[\"model_dropout_rate\"]\n",
    "        top_k_ratio = model_params[\"model_top_k_ratio\"]\n",
    "        self.top_k_every_n = model_params[\"model_top_k_every_n\"]\n",
    "        dense_neurons = model_params[\"model_dense_neurons\"]\n",
    "        edge_dim = model_params[\"model_edge_dim\"]\n",
    "\n",
    "        self.conv_layers = ModuleList([])\n",
    "        self.transf_layers = ModuleList([])\n",
    "        self.pooling_layers = ModuleList([])\n",
    "        self.bn_layers = ModuleList([])\n",
    "\n",
    "        # Transformation layer\n",
    "        self.conv1 = TransformerConv(feature_size, \n",
    "                                    embedding_size, \n",
    "                                    heads=n_heads, \n",
    "                                    dropout=dropout_rate,\n",
    "                                    edge_dim=edge_dim,\n",
    "                                    beta=True) \n",
    "\n",
    "        self.transf1 = Linear(embedding_size*n_heads, embedding_size)\n",
    "        self.bn1 = BatchNorm1d(embedding_size)\n",
    "\n",
    "        # Other layers\n",
    "        for i in range(self.n_layers):\n",
    "            self.conv_layers.append(TransformerConv(embedding_size, \n",
    "                                                    embedding_size, \n",
    "                                                    heads=n_heads, \n",
    "                                                    dropout=dropout_rate,\n",
    "                                                    edge_dim=edge_dim,\n",
    "                                                    beta=True))\n",
    "\n",
    "            self.transf_layers.append(Linear(embedding_size*n_heads, embedding_size))\n",
    "            self.bn_layers.append(BatchNorm1d(embedding_size))\n",
    "            if i % self.top_k_every_n == 0:\n",
    "                self.pooling_layers.append(TopKPooling(embedding_size, ratio=top_k_ratio))\n",
    "            \n",
    "\n",
    "        # Linear layers\n",
    "        self.linear1 = Linear(embedding_size*2, dense_neurons)\n",
    "        self.linear2 = Linear(dense_neurons, int(dense_neurons/2))  \n",
    "        self.linear3 = Linear(int(dense_neurons/2), 1)  \n",
    "\n",
    "    def forward(self, x, edge_attr, edge_index, batch_index):\n",
    "        # Initial transformation\n",
    "        x = self.conv1(x, edge_index, edge_attr)\n",
    "        x = torch.relu(self.transf1(x))\n",
    "        x = self.bn1(x)\n",
    "\n",
    "        # Holds the intermediate graph representations\n",
    "        global_representation = []\n",
    "\n",
    "        for i in range(self.n_layers):\n",
    "            x = self.conv_layers[i](x, edge_index, edge_attr)\n",
    "            x = torch.relu(self.transf_layers[i](x))\n",
    "            x = self.bn_layers[i](x)\n",
    "            # Always aggregate last layer\n",
    "            if i % self.top_k_every_n == 0 or i == self.n_layers:\n",
    "                x , edge_index, edge_attr, batch_index, _, _ = self.pooling_layers[int(i/self.top_k_every_n)](\n",
    "                    x, edge_index, edge_attr, batch_index\n",
    "                    )\n",
    "                # Add current representation\n",
    "                global_representation.append(torch.cat([gmp(x, batch_index), gap(x, batch_index)], dim=1))\n",
    "    \n",
    "        x = sum(global_representation)\n",
    "\n",
    "        # Output block\n",
    "        x = torch.relu(self.linear1(x))\n",
    "        x = F.dropout(x, p=0.8, training=self.training)\n",
    "        x = torch.relu(self.linear2(x))\n",
    "        x = F.dropout(x, p=0.8, training=self.training)\n",
    "        x = self.linear3(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "conv.TransformerConv: https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.TransformerConv.html#torch_geometric.nn.conv.TransformerConv \n",
    "To check other layers: https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html\n",
    "\n",
    "Transformer is a transduction model. These models are a type of neural network that processes an input sequence and generates an output sequence. They are used in machine learning and are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism.\n",
    "\n",
    "Transformer has an encoder-decoder structureth, where encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence of continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output sequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive, consuming the previously generated symbols as additional input when generating the next.\n",
    "\n",
    "heads: Number of multi-head-attentions. Multi-head attention is used to allow the model to jointly attend to information from different representation sub-spaces at different viewpoints. In practice, given the same set of queries, keys, and values (for example, query: node1, key: node2, value: edge value between node1 and node2 sets could be node features) we may want our model to combine knowledge from different behaviors of the same attention mechanism, such as capturing dependencies of various ranges (e.g., dependencies between nodes with different distances, like neighbor nodes or nodes connected with a 3-hop path) within a sequence. Thus, it may be beneficial to allow our attention mechanism to jointly use different representation subspaces of queries, keys, and values. To this end, instead of performing a single attention pooling, queries, keys, and values can be transformed with h independently learned linear projections. Then these h projected queries, keys, and values are fed into attention pooling in parallel. In the end, h attention pooling outputs are concatenated and transformed with another learned linear projection to produce the final output. This design is called multi-head attention, where each of the h attention pooling outputs is a head. For more details see the following links:\n",
    "- https://d2l.djl.ai/chapter_attention-mechanisms/multihead-attention.html\n",
    "- Attention is all you need: https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\n",
    "- GNN Project #3.2 - Graph Transformer\n",
    "\n",
    "dropout: Dropout probability of the normalized attention coefficients which exposes each node to a stochastically sampled neighborhood during training. (default: 0)\n",
    "\n",
    "beta: It enables some weighted aggregation.\n",
    "\n",
    "TopKPooling: Keeping K nodes of the input graph for another round of message passing and aggregation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# from mlflow.models.signature import ModelSignature\n",
    "# from mlflow.types.schema import Schema, TensorSpec\n",
    "\n",
    "BEST_PARAMETERS = {\n",
    "    \"batch_size\": 128,\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"weight_decay\": 0.0001,\n",
    "    \"sgd_momentum\": 0.8,\n",
    "    \"scheduler_gamma\": 0.8,\n",
    "    \"pos_weight\": 1.3,\n",
    "    \"model_embedding_size\": 64,\n",
    "    \"model_attention_heads\": 3,\n",
    "    \"model_layers\": 4,\n",
    "    \"model_dropout_rate\": 0.2,\n",
    "    \"model_top_k_ratio\": 0.5,\n",
    "    \"model_top_k_every_n\": 10,\n",
    "    \"model_dense_neurons\": 256\n",
    "}\n",
    "\n",
    "# input_schema = Schema([TensorSpec(np.dtype(np.float32), (-1, 30), name=\"x\"), \n",
    "#                        TensorSpec(np.dtype(np.float32), (-1, 11), name=\"edge_attr\"), \n",
    "#                        TensorSpec(np.dtype(np.int32), (2, -1), name=\"edge_index\"), \n",
    "#                        TensorSpec(np.dtype(np.int32), (-1, 1), name=\"batch_index\")])\n",
    "\n",
    "# output_schema = Schema([TensorSpec(np.dtype(np.float32), (-1, 1))])\n",
    "\n",
    "# SIGNATURE = ModelSignature(inputs=input_schema, outputs=output_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Loading model...\n",
      "Number of parameters: 340673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 560/560 [01:38<00:00,  5.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Confusion matrix: \n",
      " [[10318  6478]\n",
      " [25532 29306]]\n",
      "F1 Score: 0.6467745139149433\n",
      "Accuracy: 0.5531451545355558\n",
      "Precision: 0.5344104453116452\n",
      "Recall: 0.8189693717862732\n",
      "ROC AUC: 0.5533898462836526\n",
      "Epoch 0 | Train Loss 0.7609472795256547\n",
      "[[0.44718498]\n",
      " [0.44742325]\n",
      " [0.46465796]\n",
      " [0.5130902 ]\n",
      " [0.4682292 ]\n",
      " [0.4564873 ]\n",
      " [0.4450713 ]\n",
      " [0.5505295 ]\n",
      " [0.44805464]\n",
      " [0.46213028]]\n",
      "[0. 0. 0. 1. 0. 0. 0. 1. 0. 0.]\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "\n",
      " Confusion matrix: \n",
      " [[3172   79]\n",
      " [ 662   86]]\n",
      "F1 Score: 0.18838992332968235\n",
      "Accuracy: 0.8147036759189797\n",
      "Precision: 0.11497326203208556\n",
      "Recall: 0.5212121212121212\n",
      "ROC AUC: 0.6742732489211363\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/HIV/images/cm_0.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 172\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFinishing training with best test loss: \u001b[39m\u001b[39m{\u001b[39;00mbest_loss\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    170\u001b[0m     \u001b[39mreturn\u001b[39;00m [best_loss]\n\u001b[0;32m--> 172\u001b[0m run_one_training([BEST_PARAMETERS])\n",
      "Cell \u001b[0;32mIn[50], line 152\u001b[0m, in \u001b[0;36mrun_one_training\u001b[0;34m(params)\u001b[0m\n\u001b[1;32m    150\u001b[0m model\u001b[39m.\u001b[39meval()\n\u001b[1;32m    151\u001b[0m \u001b[39mif\u001b[39;00m epoch \u001b[39m%\u001b[39m \u001b[39m5\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 152\u001b[0m     loss \u001b[39m=\u001b[39m test(epoch, model, test_loader, loss_fn)\n\u001b[1;32m    153\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m}\u001b[39;00m\u001b[39m | Test Loss \u001b[39m\u001b[39m{\u001b[39;00mloss\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    154\u001b[0m     \u001b[39m# mlflow.log_metric(key=\"Test loss\", value=float(loss), step=epoch)\u001b[39;00m\n\u001b[1;32m    155\u001b[0m     \n\u001b[1;32m    156\u001b[0m     \u001b[39m# Update best loss\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[50], line 72\u001b[0m, in \u001b[0;36mtest\u001b[0;34m(epoch, model, test_loader, loss_fn)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[39mprint\u001b[39m(all_labels[:\u001b[39m10\u001b[39m])\n\u001b[1;32m     71\u001b[0m calculate_metrics(all_preds, all_labels, epoch, \u001b[39m\"\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 72\u001b[0m log_conf_matrix(all_preds, all_labels, epoch)\n\u001b[1;32m     73\u001b[0m \u001b[39mreturn\u001b[39;00m running_loss\u001b[39m/\u001b[39mstep\n",
      "Cell \u001b[0;32mIn[50], line 82\u001b[0m, in \u001b[0;36mlog_conf_matrix\u001b[0;34m(y_pred, y_true, epoch)\u001b[0m\n\u001b[1;32m     80\u001b[0m plt\u001b[39m.\u001b[39mfigure(figsize \u001b[39m=\u001b[39m (\u001b[39m10\u001b[39m,\u001b[39m7\u001b[39m))\n\u001b[1;32m     81\u001b[0m cfm_plot \u001b[39m=\u001b[39m sns\u001b[39m.\u001b[39mheatmap(df_cfm, annot\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, cmap\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mBlues\u001b[39m\u001b[39m'\u001b[39m, fmt\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mg\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 82\u001b[0m cfm_plot\u001b[39m.\u001b[39;49mfigure\u001b[39m.\u001b[39;49msavefig(\u001b[39mf\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mdata/HIV/images/cm_\u001b[39;49m\u001b[39m{\u001b[39;49;00mepoch\u001b[39m}\u001b[39;49;00m\u001b[39m.png\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/GNN/lib/python3.11/site-packages/matplotlib/figure.py:3343\u001b[0m, in \u001b[0;36mFigure.savefig\u001b[0;34m(self, fname, transparent, **kwargs)\u001b[0m\n\u001b[1;32m   3339\u001b[0m     \u001b[39mfor\u001b[39;00m ax \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxes:\n\u001b[1;32m   3340\u001b[0m         stack\u001b[39m.\u001b[39menter_context(\n\u001b[1;32m   3341\u001b[0m             ax\u001b[39m.\u001b[39mpatch\u001b[39m.\u001b[39m_cm_set(facecolor\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mnone\u001b[39m\u001b[39m'\u001b[39m, edgecolor\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mnone\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[0;32m-> 3343\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcanvas\u001b[39m.\u001b[39;49mprint_figure(fname, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/GNN/lib/python3.11/site-packages/matplotlib/backend_bases.py:2366\u001b[0m, in \u001b[0;36mFigureCanvasBase.print_figure\u001b[0;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, pad_inches, bbox_extra_artists, backend, **kwargs)\u001b[0m\n\u001b[1;32m   2362\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   2363\u001b[0m     \u001b[39m# _get_renderer may change the figure dpi (as vector formats\u001b[39;00m\n\u001b[1;32m   2364\u001b[0m     \u001b[39m# force the figure dpi to 72), so we need to set it again here.\u001b[39;00m\n\u001b[1;32m   2365\u001b[0m     \u001b[39mwith\u001b[39;00m cbook\u001b[39m.\u001b[39m_setattr_cm(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfigure, dpi\u001b[39m=\u001b[39mdpi):\n\u001b[0;32m-> 2366\u001b[0m         result \u001b[39m=\u001b[39m print_method(\n\u001b[1;32m   2367\u001b[0m             filename,\n\u001b[1;32m   2368\u001b[0m             facecolor\u001b[39m=\u001b[39;49mfacecolor,\n\u001b[1;32m   2369\u001b[0m             edgecolor\u001b[39m=\u001b[39;49medgecolor,\n\u001b[1;32m   2370\u001b[0m             orientation\u001b[39m=\u001b[39;49morientation,\n\u001b[1;32m   2371\u001b[0m             bbox_inches_restore\u001b[39m=\u001b[39;49m_bbox_inches_restore,\n\u001b[1;32m   2372\u001b[0m             \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   2373\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m   2374\u001b[0m     \u001b[39mif\u001b[39;00m bbox_inches \u001b[39mand\u001b[39;00m restore_bbox:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/GNN/lib/python3.11/site-packages/matplotlib/backend_bases.py:2232\u001b[0m, in \u001b[0;36mFigureCanvasBase._switch_canvas_and_return_print_method.<locals>.<lambda>\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   2228\u001b[0m     optional_kws \u001b[39m=\u001b[39m {  \u001b[39m# Passed by print_figure for other renderers.\u001b[39;00m\n\u001b[1;32m   2229\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mdpi\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mfacecolor\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39medgecolor\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39morientation\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   2230\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mbbox_inches_restore\u001b[39m\u001b[39m\"\u001b[39m}\n\u001b[1;32m   2231\u001b[0m     skip \u001b[39m=\u001b[39m optional_kws \u001b[39m-\u001b[39m {\u001b[39m*\u001b[39minspect\u001b[39m.\u001b[39msignature(meth)\u001b[39m.\u001b[39mparameters}\n\u001b[0;32m-> 2232\u001b[0m     print_method \u001b[39m=\u001b[39m functools\u001b[39m.\u001b[39mwraps(meth)(\u001b[39mlambda\u001b[39;00m \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: meth(\n\u001b[1;32m   2233\u001b[0m         \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m{k: v \u001b[39mfor\u001b[39;49;00m k, v \u001b[39min\u001b[39;49;00m kwargs\u001b[39m.\u001b[39;49mitems() \u001b[39mif\u001b[39;49;00m k \u001b[39mnot\u001b[39;49;00m \u001b[39min\u001b[39;49;00m skip}))\n\u001b[1;32m   2234\u001b[0m \u001b[39melse\u001b[39;00m:  \u001b[39m# Let third-parties do as they see fit.\u001b[39;00m\n\u001b[1;32m   2235\u001b[0m     print_method \u001b[39m=\u001b[39m meth\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/GNN/lib/python3.11/site-packages/matplotlib/backends/backend_agg.py:509\u001b[0m, in \u001b[0;36mFigureCanvasAgg.print_png\u001b[0;34m(self, filename_or_obj, metadata, pil_kwargs)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprint_png\u001b[39m(\u001b[39mself\u001b[39m, filename_or_obj, \u001b[39m*\u001b[39m, metadata\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, pil_kwargs\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    463\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    464\u001b[0m \u001b[39m    Write the figure to a PNG file.\u001b[39;00m\n\u001b[1;32m    465\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    507\u001b[0m \u001b[39m        *metadata*, including the default 'Software' key.\u001b[39;00m\n\u001b[1;32m    508\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 509\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_print_pil(filename_or_obj, \u001b[39m\"\u001b[39;49m\u001b[39mpng\u001b[39;49m\u001b[39m\"\u001b[39;49m, pil_kwargs, metadata)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/GNN/lib/python3.11/site-packages/matplotlib/backends/backend_agg.py:458\u001b[0m, in \u001b[0;36mFigureCanvasAgg._print_pil\u001b[0;34m(self, filename_or_obj, fmt, pil_kwargs, metadata)\u001b[0m\n\u001b[1;32m    453\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    454\u001b[0m \u001b[39mDraw the canvas, then save it using `.image.imsave` (to which\u001b[39;00m\n\u001b[1;32m    455\u001b[0m \u001b[39m*pil_kwargs* and *metadata* are forwarded).\u001b[39;00m\n\u001b[1;32m    456\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    457\u001b[0m FigureCanvasAgg\u001b[39m.\u001b[39mdraw(\u001b[39mself\u001b[39m)\n\u001b[0;32m--> 458\u001b[0m mpl\u001b[39m.\u001b[39;49mimage\u001b[39m.\u001b[39;49mimsave(\n\u001b[1;32m    459\u001b[0m     filename_or_obj, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbuffer_rgba(), \u001b[39mformat\u001b[39;49m\u001b[39m=\u001b[39;49mfmt, origin\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mupper\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    460\u001b[0m     dpi\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfigure\u001b[39m.\u001b[39;49mdpi, metadata\u001b[39m=\u001b[39;49mmetadata, pil_kwargs\u001b[39m=\u001b[39;49mpil_kwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/GNN/lib/python3.11/site-packages/matplotlib/image.py:1689\u001b[0m, in \u001b[0;36mimsave\u001b[0;34m(fname, arr, vmin, vmax, cmap, format, origin, dpi, metadata, pil_kwargs)\u001b[0m\n\u001b[1;32m   1687\u001b[0m pil_kwargs\u001b[39m.\u001b[39msetdefault(\u001b[39m\"\u001b[39m\u001b[39mformat\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mformat\u001b[39m)\n\u001b[1;32m   1688\u001b[0m pil_kwargs\u001b[39m.\u001b[39msetdefault(\u001b[39m\"\u001b[39m\u001b[39mdpi\u001b[39m\u001b[39m\"\u001b[39m, (dpi, dpi))\n\u001b[0;32m-> 1689\u001b[0m image\u001b[39m.\u001b[39;49msave(fname, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mpil_kwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/GNN/lib/python3.11/site-packages/PIL/Image.py:2428\u001b[0m, in \u001b[0;36mImage.save\u001b[0;34m(self, fp, format, **params)\u001b[0m\n\u001b[1;32m   2426\u001b[0m         fp \u001b[39m=\u001b[39m builtins\u001b[39m.\u001b[39mopen(filename, \u001b[39m\"\u001b[39m\u001b[39mr+b\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   2427\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 2428\u001b[0m         fp \u001b[39m=\u001b[39m builtins\u001b[39m.\u001b[39;49mopen(filename, \u001b[39m\"\u001b[39;49m\u001b[39mw+b\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m   2430\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   2431\u001b[0m     save_handler(\u001b[39mself\u001b[39m, fp, filename)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/HIV/images/cm_0.png'"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x700 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch \n",
    "from torch_geometric.data import DataLoader\n",
    "from sklearn.metrics import confusion_matrix, f1_score, accuracy_score, precision_score, recall_score, roc_auc_score\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "# import mlflow.pytorch\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "import pandas as pd \n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def train_one_epoch(epoch, model, train_loader, optimizer, loss_fn):\n",
    "    # Enumerate over the data\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    running_loss = 0.0\n",
    "    step = 0\n",
    "    for _, batch in enumerate(tqdm(train_loader)):\n",
    "        # Use GPU\n",
    "        batch.to(device)  \n",
    "        # Reset gradients\n",
    "        optimizer.zero_grad() \n",
    "        # Passing the node features and the connection info\n",
    "        pred = model(batch.x.float(), \n",
    "                                batch.edge_attr.float(),\n",
    "                                batch.edge_index, \n",
    "                                batch.batch) \n",
    "        # Calculating the loss and gradients\n",
    "        loss = loss_fn(torch.squeeze(pred), batch.y.float())\n",
    "        loss.backward()  \n",
    "        optimizer.step()  \n",
    "        # Update tracking\n",
    "        running_loss += loss.item()\n",
    "        step += 1\n",
    "        all_preds.append(np.rint(torch.sigmoid(pred).cpu().detach().numpy()))\n",
    "        all_labels.append(batch.y.cpu().detach().numpy())\n",
    "    all_preds = np.concatenate(all_preds).ravel()\n",
    "    all_labels = np.concatenate(all_labels).ravel()\n",
    "    calculate_metrics(all_preds, all_labels, epoch, \"train\")\n",
    "    return running_loss/step\n",
    "\n",
    "def test(epoch, model, test_loader, loss_fn):\n",
    "    all_preds = []\n",
    "    all_preds_raw = []\n",
    "    all_labels = []\n",
    "    running_loss = 0.0\n",
    "    step = 0\n",
    "    for batch in test_loader:\n",
    "        batch.to(device)  \n",
    "        pred = model(batch.x.float(), \n",
    "                        batch.edge_attr.float(),\n",
    "                        batch.edge_index, \n",
    "                        batch.batch) \n",
    "        loss = loss_fn(torch.squeeze(pred), batch.y.float())\n",
    "\n",
    "         # Update tracking\n",
    "        running_loss += loss.item()\n",
    "        step += 1\n",
    "        all_preds.append(np.rint(torch.sigmoid(pred).cpu().detach().numpy()))\n",
    "        all_preds_raw.append(torch.sigmoid(pred).cpu().detach().numpy())\n",
    "        all_labels.append(batch.y.cpu().detach().numpy())\n",
    "    \n",
    "    all_preds = np.concatenate(all_preds).ravel()\n",
    "    all_labels = np.concatenate(all_labels).ravel()\n",
    "    print(all_preds_raw[0][:10])\n",
    "    print(all_preds[:10])\n",
    "    print(all_labels[:10])\n",
    "    calculate_metrics(all_preds, all_labels, epoch, \"test\")\n",
    "    log_conf_matrix(all_preds, all_labels, epoch)\n",
    "    return running_loss/step\n",
    "\n",
    "def log_conf_matrix(y_pred, y_true, epoch):\n",
    "    # Log confusion matrix as image\n",
    "    cm = confusion_matrix(y_pred, y_true)\n",
    "    classes = [\"0\", \"1\"]\n",
    "    df_cfm = pd.DataFrame(cm, index = classes, columns = classes)\n",
    "    plt.figure(figsize = (10,7))\n",
    "    cfm_plot = sns.heatmap(df_cfm, annot=True, cmap='Blues', fmt='g')\n",
    "    cfm_plot.figure.savefig(f'data/HIV/images/cm_{epoch}.png')\n",
    "    # mlflow.log_artifact(f\"data/images/cm_{epoch}.png\")\n",
    "\n",
    "def calculate_metrics(y_pred, y_true, epoch, type):\n",
    "    print(f\"\\n Confusion matrix: \\n {confusion_matrix(y_pred, y_true)}\")\n",
    "    print(f\"F1 Score: {f1_score(y_true, y_pred)}\")\n",
    "    print(f\"Accuracy: {accuracy_score(y_true, y_pred)}\")\n",
    "    prec = precision_score(y_true, y_pred)\n",
    "    rec = recall_score(y_true, y_pred)\n",
    "    print(f\"Precision: {prec}\")\n",
    "    print(f\"Recall: {rec}\")\n",
    "    # mlflow.log_metric(key=f\"Precision-{type}\", value=float(prec), step=epoch)\n",
    "    # mlflow.log_metric(key=f\"Recall-{type}\", value=float(rec), step=epoch)\n",
    "    try:\n",
    "        roc = roc_auc_score(y_true, y_pred)\n",
    "        print(f\"ROC AUC: {roc}\")\n",
    "        # mlflow.log_metric(key=f\"ROC-AUC-{type}\", value=float(roc), step=epoch)\n",
    "    except:\n",
    "        # mlflow.log_metric(key=f\"ROC-AUC-{type}\", value=float(0), step=epoch)\n",
    "        print(f\"ROC AUC: notdefined\")\n",
    "\n",
    "def run_one_training(params):\n",
    "    params = params[0]\n",
    "    # with mlflow.start_run() as run:\n",
    "    # Log parameters used in this experiment\n",
    "    # for key in params.keys():\n",
    "    #     mlflow.log_param(key, params[key])\n",
    "\n",
    "    # Loading the dataset\n",
    "    print(\"Loading dataset...\")\n",
    "    train_dataset = MoleculeDataset(root=\"data/HIV/\", filename=\"HIV_train_oversampled.csv\")\n",
    "    test_dataset = MoleculeDataset(root=\"data/HIV/\", filename=\"HIV_test.csv\", test=True)\n",
    "    params[\"model_edge_dim\"] = train_dataset[0].edge_attr.shape[1]\n",
    "\n",
    "    # Prepare training\n",
    "    train_loader = DataLoader(train_dataset, batch_size=params[\"batch_size\"], shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=params[\"batch_size\"], shuffle=True)\n",
    "\n",
    "    # Loading the model\n",
    "    print(\"Loading model...\")\n",
    "    model_params = {k: v for k, v in params.items() if k.startswith(\"model_\")}\n",
    "    model = GNN(feature_size=train_dataset[0].x.shape[1], model_params=model_params) \n",
    "    model = model.to(device)\n",
    "    print(f\"Number of parameters: {count_parameters(model)}\")\n",
    "    # mlflow.log_param(\"num_params\", count_parameters(model))\n",
    "\n",
    "\n",
    "    # < 1 increases precision, > 1 recall\n",
    "    weight = torch.tensor([params[\"pos_weight\"]], dtype=torch.float32).to(device)\n",
    "    loss_fn = torch.nn.BCEWithLogitsLoss(pos_weight=weight)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), \n",
    "                                lr=params[\"learning_rate\"],\n",
    "                                momentum=params[\"sgd_momentum\"],\n",
    "                                weight_decay=params[\"weight_decay\"])\n",
    "    # scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=params[\"scheduler_gamma\"])\n",
    "    \n",
    "    # Start training\n",
    "    best_loss = 1000\n",
    "    early_stopping_counter = 0\n",
    "    for epoch in range(300): \n",
    "        if early_stopping_counter <= 10: # = x * 5 \n",
    "            # Training\n",
    "            model.train()\n",
    "            loss = train_one_epoch(epoch, model, train_loader, optimizer, loss_fn)\n",
    "            print(f\"Epoch {epoch} | Train Loss {loss}\")\n",
    "            # mlflow.log_metric(key=\"Train loss\", value=float(loss), step=epoch)\n",
    "\n",
    "            # Testing\n",
    "            model.eval()\n",
    "            if epoch % 5 == 0:\n",
    "                loss = test(epoch, model, test_loader, loss_fn)\n",
    "                print(f\"Epoch {epoch} | Test Loss {loss}\")\n",
    "                # mlflow.log_metric(key=\"Test loss\", value=float(loss), step=epoch)\n",
    "                \n",
    "                # Update best loss\n",
    "                if float(loss) < best_loss:\n",
    "                    best_loss = loss\n",
    "                    # Save the currently best model \n",
    "                    # mlflow.pytorch.log_model(model, \"model\", signature=SIGNATURE)\n",
    "                    early_stopping_counter = 0\n",
    "                else:\n",
    "                    early_stopping_counter += 1\n",
    "\n",
    "            # scheduler.step()\n",
    "        else:\n",
    "            print(\"Early stopping due to no improvement.\")\n",
    "            return [best_loss]\n",
    "    print(f\"Finishing training with best test loss: {best_loss}\")\n",
    "    return [best_loss]\n",
    "\n",
    "run_one_training([BEST_PARAMETERS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GNN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
